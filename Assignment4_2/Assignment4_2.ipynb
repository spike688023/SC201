{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9085,
     "status": "ok",
     "timestamp": 1615284098319,
     "user": {
      "displayName": "stanCode標準程式教育機構",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEXNEf8CuikK9lo_S6uoGBFySnbTpVXF0hGf8r=s64",
      "userId": "17922542024282624619"
     },
     "user_tz": -480
    },
    "id": "EbHLo8DVukj0",
    "outputId": "a94ed005-098a-4727-a1e2-f6cd070a4840"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# 請輸入資料夾之所在位置\n",
    "# e.g. 'Colab\\ Notebooks/SC201Assignment4_2'\n",
    "FOLDERNAME = 'Colab\\ Notebooks/SC201Assignment4_2'\n",
    "\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "files_loc = '/content/drive/MyDrive/{}'.format(FOLDERNAME)\n",
    "sys.path.append(files_loc)\n",
    "\n",
    "# this downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd drive/MyDrive/$FOLDERNAME/sc201/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JbIhKDEZp2U"
   },
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvLTfbs0Zp2V"
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60XxL8FrZp2V"
   },
   "source": [
    "大家之前遇到的 classification problem 是所謂的二元分類 binary classification，例如：將影評歸類為好評或差評。但並不是所有東西都適合做二元分類。要是有多種 classes，這時候應該如何進行分類？\n",
    "\n",
    "Multiclass classification 的任務可以透過 neural network 來完成。大家會在這份作業中寫出一套 fully-connected neural network，並將其套用在 CIFAR-10 圖像資料庫上。CIFAR-10 資料庫顧名思義有十種類別的圖像，詳細介紹請參考：https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "我們在 `classifiers/neural_net.py` 中定義了 `TwoLayerNet` 類別以實現這套神經網路。神經網路的架構如下：\n",
    "- Input layer：這是我們的 data，例如：圖像中的所有畫素。我們用 $D$ 表示每筆 data 的長度 (畫素的數量)，$N$ 表示 data 的數量 (圖像的數量)。\n",
    "- Hidden layer：接收 data 的第一層 fully connected layer，權重 weight 和偏差 bias 名為 W1 和 b1。假設 hidden layer 有 $H$ 個神經元，那麼 W1 和 b1 的大小分別為 $(D, H)$ 和 $(H,)$。\n",
    "- ReLU：常見的 activation function 之一，使這套網路變成非線性的必要元素。ReLU 會過濾掉 hidden layer 的 $H$ 個神經元中任何小於零的 neuron。\n",
    "- Output layer：接收 ReLU 輸出的第二層 fully connected layer，權重和偏差名為 W2 和 b2，大小分別為 $(H, C)$ 和 $(C,)$。我們用 $C$ 表示類別的數目。\n",
    "\n",
    "神經網路除了最底層的 input，共有兩層神經元，故稱作 `TwoLayerNet`。這兩層神經元的 W1、b1、W2 和 b2 參數儲存於 `self.params` 字典，字典的 key 是參數名稱，value 是 numpy 矩陣。\n",
    "\n",
    "以 CIFAR-10 當作 data 為例，每張圖像有 $32 \\times 32 \\times 3 = 3072$ 個畫素，且共有十種圖像類別，所以 $D = 3072$，$C = 10$，$N$ 和 $H$ 可以由我們任意決定。\n",
    "\n",
    "Output layer 輸出的 $C$ 個數值，是不是與我們的 $C$ 種類別有所關聯呢？是的，output layer 輸出的結果我們稱之為 class scores，以 $f_j | j = 0, 1, \\ldots, C-1$ 表示，每一個數值 $f_j$ 是 data 被歸類為第 $j$ 類別之分數，分數最高的就是最有可能的類別。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1q1_-Nm4Zp2W"
   },
   "source": [
    "## Softmax Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z90IyKjdZp2W"
   },
   "source": [
    "我們現在知道如何用這套模型做 prediction，但是要先訓練過模型才能夠做預測。這時候就要定義 cost function，並反覆執行 forward 與 backward pass 做 gradient descent，以找出 cost function 的最低點。我們可以從 binary classification 的 loss function 推導出一個適合 multiclass classification 的 cost function。\n",
    "\n",
    "之前在 binary classification (i.e. $C = 2$，logistic regression) 所使用的 loss function 為：\n",
    "\n",
    "$$L^{(i)} = \\begin{cases} \n",
    "- \\log (1 - h^{(i)}) & \\quad \\text{if } y^{(i)} = 0 \\\\\n",
    "- \\log h^{(i)} & \\quad \\text{if } y^{(i)} = 1\n",
    "\\end{cases} $$\n",
    "\n",
    "$y^{(i)}$ 是第 $i$ 筆 data 的正確類別，$0 \\leq i \\leq N - 1$。而 $0 \\leq h^{(i)} \\leq 1$ 是模型的預測，也就是 data 被歸類為類別 $1$ 的機率，$1 - h^{(i)}$ 則是 data 被歸類為類別 $0$ 的機率。假設我們使用上面的神經網路取得 class scores $f_0^{(i)}$ 和 $f_1^{(i)}$，那這些分數跟機率 $1 - h^{(i)}$ 和 $h^{(i)}$ 又有什麼關係？畢竟 output layer 輸出的數值不一定介於 $0$ 跟 $1$ 之間，所以我們不能直接把 class score 當成是機率。但是我們可以使用所謂的 softmax function，將 class scores 標準化 (normalization)，使它們的和為 $1$：\n",
    "\n",
    "$$ \\sigma(f_j^{(i)}) = \\frac{\\exp (f_j^{(i)})}{\\sum_{k=0}^{C-1} \\exp (f_k^{(i)})} $$\n",
    "\n",
    "例如：當 $C = 2$ 時，$ \\sigma(f_0^{(i)}) = \\frac{\\exp (f_0^{(i)})}{\\exp (f_0^{(i)}) + \\exp (f_1^{(i)})} $，$ \\sigma(f_1^{(i)}) = \\frac{\\exp (f_1^{(i)})}{\\exp (f_0^{(i)}) + \\exp (f_1^{(i)})} $。大家可以自行驗證 $ 0 \\leq \\sigma(f_j^{(i)}) \\leq 1 $，$ \\sum_{j=0}^{C-1} \\sigma(f_j^{(i)}) = 1 $。\n",
    "\n",
    "這時候我們就可以將 $\\sigma(f_j^{(i)})$ 視成 data 屬於第 $j$ 個類別的機率！\n",
    "\n",
    "所以 $1 - h^{(i)} = \\sigma(f_0^{(i)})$，$h^{(i)} = \\sigma(f_1^{(i)})$。我們可以重整 binary loss function 如下：\n",
    "\n",
    "$$ L^{(i)} = - \\log \\sigma(f_y^{(i)}) = - \\log \\left ( \\frac{\\exp (f_y^{(i)})}{\\sum_{k=0}^1 \\exp (f_k^{(i)})} \\right ) $$\n",
    "\n",
    "這樣大家就應該猜得出來 multiclass loss function 的公式了吧！\n",
    "\n",
    "$$ L^{(i)} = - \\log \\sigma(f_y^{(i)}) = - \\log \\left ( \\frac{\\exp (f_y^{(i)})}{\\sum_{k=0}^{C-1} \\exp (f_k^{(i)})} \\right ) $$\n",
    "\n",
    "最後，我們將所有 data 的 loss 加起來取平均，得到 multiclass cost function：\n",
    "\n",
    "$$ J = \\frac{1}{N} \\sum_i L^{(i)} $$\n",
    "\n",
    "當我們的模型能夠更準確的預測出第 $i$ 筆 data 的所屬類別 $y^{(i)}$ 時，$y^{(i)}$ 的 class score $f_y^{(i)}$ 會比其他 $f_j^{(i)} | j \\neq y^{(i)}$ 來的大，loss $L^{(i)}$ 也會跟著下降，反之亦然。所以只要我們對於 $J$ 做 gradient descent，模型就會進步！\n",
    "\n",
    "這個 loss function 公式我們稱作 softmax loss。有了 softmax loss，就可以訓練出強大的 multiclass 模型，然後拿來做預測！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI-RMLuVukj0",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Implementing a Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDdfiEAKukj1",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%cd drive/MyDrive/$FOLDERNAME\n",
    "from sc201.classifiers.neural_net import TwoLayerNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfz68KWjukj1",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "撰寫程式的過程中，我們先將 CIFAR-10 擱置在旁，暫時使用虛構的 toy data 來做測試。我們透由下方的 code 來建立一組 toy data 並初始化大家接下來會完成的 toy model。注意 toy data 和 toy model 對應的矩陣大小不一定跟 CIFAR-10 的一樣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqL8Zilyukj1",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Poe7VTkwukj1"
   },
   "source": [
    "# Forward pass: compute scores\n",
    "請開啟 `classifiers/neural_net.py`。檔案中的 `TwoLayerNet.cost` 函數：\n",
    "- Input 為 training data 和 `self.params` 中的 weights 和 biases、\n",
    "- Output 為 class scores，或是 cost 和 gradients。\n",
    "\n",
    "請寫出 forward pass 的第一步驟，利用神經網路的 weights 和 biases 計算出 class scores。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSnswUpdukj1"
   },
   "outputs": [],
   "source": [
    "scores = net.cost(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ2Qyk4lukj1"
   },
   "source": [
    "# Forward pass: compute loss\n",
    "接下來請寫出 forward pass 的第二步驟，計算出 softmax loss 及總和 cost。\n",
    "\n",
    "### Hint 1：###\n",
    "\n",
    "在計算\n",
    "\n",
    "$$ \\sigma(f_y^{(i)}) = \\frac{\\exp (f_y^{(i)})}{\\sum_{k=0}^{C-1} \\exp (f_k^{(i)})} $$\n",
    "\n",
    "的時候，如果 neural network 輸出的 class scores 很大，$e^{f_y^{(i)}}$ 和 $\\sum_{k=0}^{C-1} e^{f_k^{(i)}}$ 也會跟著變大，電腦相除時可能會因為不穩定而出錯。這時候，我們可以從每個 class score 扣除固定的常數 $A$：\n",
    "\n",
    "$$ \\sigma(f_y^{(i)}) = \\frac{\\exp (f_y^{(i)} - A)}{\\sum_{k=0}^{C-1} \\exp (f_k^{(i)} - A)} $$\n",
    "\n",
    "大家可以自行確認兩條式子是相等的。\n",
    "\n",
    "$A$ 可以由我們任意決定。我們通常會將 $A$ 設為 $\\max_k f_k^{(i)}$，也就是將 class scores 平移，使最高的分數為零。\n",
    "\n",
    "因此，大家的答案中應該會出現這一行 code：```scores -= np.amax(scores, axis = 1, keepdims = True)```\n",
    "\n",
    "### Hint 2：###\n",
    "\n",
    "另外，我們必須找出每筆資料 $i$ 的正確類別 $y^{(i)}$ 所對應的 class score $f_y^{(i)}$。這時候就可以使用以下的 numpy 語法：```scores[range(N), y]```\n",
    "\n",
    "```range(N)``` 和 ```y``` 都是長度為 $N$ 的向量。這行 code 等同於 ```np.array([scores[i][y[i]] for i in range(N)])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QMFva15ukj1"
   },
   "outputs": [],
   "source": [
    "cost, _ = net.cost(X, y)\n",
    "correct_cost = 1.265857052118\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print('Difference between your cost and correct cost:')\n",
    "print(np.sum(np.abs(cost - correct_cost)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuH87fr_ukj1"
   },
   "source": [
    "# Backward pass\n",
    "請寫出函數中剩餘的 backward pass 步驟，計算出 cost 對於 `W1`、`b1`、`W2` 和 `b2` 的梯度。我們可以利用上面完成的 forward pass 來估測 gradient 的數值，與 backward pass 的結果做相比。\n",
    "\n",
    "### Hint：###\n",
    "\n",
    "Softmax loss function 對於 class scores 的偏微分如下：\n",
    "\n",
    "$$ \\frac{\\partial L^{(i)}}{\\partial f_j^{(i)}} = \\begin{cases}\n",
    "\\sigma(f_j^{(i)}) - 1 & \\quad \\text{if } j = y^{(i)} \\\\\n",
    "\\sigma(f_j^{(i)}) & \\quad \\text{else}\n",
    "\\end{cases}$$\n",
    "\n",
    "所以 cost function 對於 class scores 的梯度應該這樣計算：\n",
    "\n",
    "```python\n",
    "sigma = np.exp(scores) / (np.sum(np.exp(scores), axis = 1, keepdims=True))\n",
    "sigma[range(N), y] -= 1\n",
    "grads_classScores = sigma / N # note: cost function has factor 1/N\n",
    "```\n",
    "\n",
    "Cost function 對於第二個偏差 b2 的梯度則是：```grads['b2'] = np.sum(grad_classScores, axis = 0)```\n",
    "\n",
    "```grads``` 字典中應儲存的其餘（較為好寫的）三個梯度就請大家自己計算了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wXQlH9Fukj1"
   },
   "outputs": [],
   "source": [
    "from sc201.gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "cost, grads = net.cost(X, y)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.cost(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3V0gXNlukj1"
   },
   "source": [
    "# Train the network\n",
    "\n",
    "Forward 和 backward pass 都完成後，就可以做 training 了。請完成：\n",
    "- `TwoLayerNet.train` 函數，執行 Minibatch Gradient Descent、\n",
    "- `TwoLayerNet.predict` 函數，讓 model 可以做出預測。\n",
    "\n",
    "記得 Minibatch Gradient Descent (MBGD) 是介於一般 Batch Gradient Descent (BGD) 和 Stochastic Gradient Descent (SGD) 之間的 learning 模式，每一次做更新是從所有資料中隨機抽取固定數量的 Minibatch（樣本可重複）。而預測則是使用 forward pass 計算出 class scores 之後，回傳最大分數之對應類別。\n",
    "\n",
    "### Hint：###\n",
    "- 當您在執行 Minibatch Gradient Descent (MBGD) 時，可以試試「先製作一個空 list，隨機取出 X[i] 或 y[i] 加入 list，最後再把 list 轉換成 numpy array」即可。\n",
    "\n",
    "- np.argmax 會在「 model 預測答案」時派上用場。歡迎參閱下方 numpy 官網連結了解該如何使用: https://numpy.org/doc/stable/reference/generated/numpy.argmax.html \n",
    "\n",
    "寫完後請執行下方的 cell 來訓練出一套可以對 toy data 做出預測的 toy model。最終的 training cost 應小於 0.02。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_training_loss"
   },
   "outputs": [],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, num_iters=100,\n",
    "            verbose=False)\n",
    "\n",
    "print('Final training cost: ', stats['cost_history'][-1])\n",
    "\n",
    "# plot the cost history\n",
    "plt.plot(stats['cost_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training cost')\n",
    "plt.title('Training Cost history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vlp2zWgukj1"
   },
   "source": [
    "# Load the data\n",
    "\n",
    "大家寫好的 neural network 類別通過了 toy data 的測試，蓄勢待發！接下來我們將 dataset 升級為 CIFAR-10，用真實的 data 訓練出強大的模型！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2v8tg-Byukj1",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "from sc201.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'sc201/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYeArQ8rukj1"
   },
   "source": [
    "# Train a network\n",
    "這次的訓練過程中，我們會在每個 epoch 結束後，將 learning rate 乘上一個固定係數，以達成指數衰減 (exponential decay)。這是為了控制 training 後期的 step size，以達成收斂。Decay 的部分已經幫大家寫好了，請直接執行下面的 cell。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdeZwm1Bukj1",
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTNP_SyFukj1"
   },
   "source": [
    "# Debug the training\n",
    "\n",
    "使用上方的預設參數的情況下，大家應該會看到 validation accuracy 大約為 0.29。這並不是很理想的結果...\n",
    "\n",
    "我們可以觀察 training 和 validation 在每個 epoch 的 cost 和 accuracy，以做診斷，或是以圖像的方式畫出 neural network 直接接收 input 的第一層 layer，也就是 hidden layer 的權重。大家或許可以從權重的圖像中看出一些輪廓...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9qPBa8yukj1"
   },
   "outputs": [],
   "source": [
    "# Plot the cost function\n",
    "plt.plot(stats['cost_history'])\n",
    "plt.title('Cost history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsY7sCg2zmB6"
   },
   "outputs": [],
   "source": [
    "# Plot the train / validation accuracies\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QL3_-dsEukj1"
   },
   "outputs": [],
   "source": [
    "from sc201.vis_utils import visualize_grid\n",
    "\n",
    "# Visualize the weights of the network\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb3WMI6xukj1"
   },
   "source": [
    "# Tune your hyperparameters\n",
    "\n",
    "### What's wrong：###\n",
    "上方的 cost 大概是以線性的方式在下降，這意味著 learning rate 可能太低。\n",
    "\n",
    "另外，training 和 validation 的準確率幾乎是一樣的，這意味著我們的 model 可能太小。\n",
    "\n",
    "(相反的，如果 training 和 validation 的準確率相差太大，就有可能是 model 太大導致 overfitting。)\n",
    "\n",
    "\n",
    "### Tuning：###\n",
    "Hyperparameter tuning 是 AI 的一大環節，我們在這裡可以調整的 hyperparameters 為：\n",
    "\n",
    "- hidden layer size $H$、\n",
    "- mini-batch size、\n",
    "- learning rate、\n",
    "- learning rate decay schedule。\n",
    "\n",
    "$\\color{red}{注意：請勿調整}$ num_iters\n",
    "\n",
    "大家可以針對上面觀測到的問題，對這些 hyperparameters 做調整，將最好的模型儲存於 `best_net`。最終的 validation accuracy 應該要達到 48% 以上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZGrXCzkukj1",
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "best_net = None # store the best model into this \n",
    "\n",
    "#################################################################################\n",
    "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
    "# model in best_net.                                                            #\n",
    "#                                                                               #\n",
    "# To help debug your network, it may help to use visualizations similar to the  #\n",
    "# ones we used above; these visualizations will have significant qualitative    #\n",
    "# differences from the ones we saw above for the poorly tuned network.          #\n",
    "#                                                                               #\n",
    "# Tweaking hyperparameters by hand can be fun, but you might find it useful to  #\n",
    "# write code to sweep through possible combinations of hyperparameters          #\n",
    "# automatically like we did on the previous exercises.                          #\n",
    "#################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "val_accuracy"
   },
   "outputs": [],
   "source": [
    "# Print your validation accuracy: this should be above 48%\n",
    "val_acc = (best_net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Be0P0_1ukj1"
   },
   "outputs": [],
   "source": [
    "# Visualize the weights of the best network\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnABFeoZukj1"
   },
   "source": [
    "# Run on the test set\n",
    "\n",
    "我們最後用 test data 來測試 `best_net`。Test accuracy 應該為 48% 以上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_accuracy"
   },
   "outputs": [],
   "source": [
    "# Print your test accuracy: this should be above 48%\n",
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQcBPWvIukj2"
   },
   "source": [
    "---\n",
    "# IMPORTANT\n",
    "恭喜大家完成作業！ **Please make sure you save this notebook `Assignment4_2.ipynb` and `neural_net.py`!**\n",
    "\n",
    "請直接分享您 SC201_Assignment4-2 $\\color{red}{資料夾的連結}$！並將連結提交至作業繳交表單"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFTtDNrvZp2d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment4_2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
